{
  "hash": "62cdba0099b484ac6aa150f9f68258f1",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"A Beginner's Guide to Machine Learning Concepts and Applications\"\nabstract: \"This tutorial introduces the fundamentals of machine learning, covering key concepts such as supervised and unsupervised learning, data labeling, and model evaluation.\"\nimage: images/intro-to-ml.webp\nformat: \n    html:\n        code-line-numbers: true\n        toc: true\n        link-external-newwindow: true\nexecute: \n    freeze: auto\n---\n\n\n## What is Machine Learning?\n\nMachine learning is a subset of artificial intelligence where computers use data to identify patterns and make predictions. These models improve over time by learning from new data and experiences.\n\nMachine learning algorithms work by identifying patterns in the data, specifically between the independent variables (features, often represented as X) and the dependent variables (targets, often represented as Y). These patterns enable the model to make predictions or classifications based on new, unseen data.\n\n## Labeled & Unlabeled Data\n\nBefore we get into machine learning, we need to talk about data. Data is the foundation of all machine learning—without it, we wouldn't be able to train the models! There are two broad categories of data used in machine learning: labeled and unlabeled data.\n\nLabeled data contains both the input and the corresponding correct output/target or dependent measure. In this example, we can see two types of labeled data—one labeled with \"cat/dog\" and another labeled with \"weight.\" In each case, the model would be trained to predict whether the input was a cat or a dog, or to estimate the weight, depending on the label provided. Unlabeled data only contains the input, leaving the model to identify patterns without guidance.\n\n![](images/labelled-unlabeled.png){width=\"60%\" fig-align=\"center\"}\n\n## Two Category of Machine Learning\n\nMachine learning can be broadly divided into two primary categories: supervised and unsupervised learning. Each approach has its own strengths and limitations, making them suitable for different types of tasks.\n\n::::::: columns\n:::: {.column width=\"50%\"}\n::: {style=\"text-align: center;\"}\n#### Supervised Learning\n:::\n-   Uses [labeled]{.highlight1} training data\n-   Majority of machine learning applications\n-   Commonly used for tasks like [classification]{.highligh1} and regression\n::::\n\n:::: {.column width=\"50%\"}\n::: {style=\"text-align: center;\"}\n#### Unsupervised Learning\n:::\n-   Uses [unlabeled]{.highlight1} training data\n-   Less common but powerful for exploratory analysis\n-   Often used for clustering, content personalization, and dimensionality reduction\n::::\n:::::::\n\n::: {style=\"text-align: center; text-decoration: underline;\"}\n### Supervised Machine Learning \n:::\n\nSupervised machine learning involves training models on labeled data, where the goal is often to make predictions or classifications based on new, unseen data.\n\n::::::: columns\n:::: {.column width=\"50%\"}\n::: {style=\"text-align: center;\"}\n#### Pros\n:::\n-   High accuracy\n    -   Ability to learn from known examples\n-   Excellent predictive power\n    -   Effective at classifying new data based on historical patterns\n::::\n\n:::: {.column width=\"50%\"}\n::: {style=\"text-align: center;\"}\n#### Cons\n:::\n-   Requires labeled data\n    -   Time-consuming and expensive to obtain\n-   Risk of overfitting\n    -   Models may become too tailored to the training data\n::::\n:::::::\n\n[Common Algorithms: Linear/Logistic Regression, Support Vector Machines, Decision Trees, K-nearest neighbours]{.footer-font}\n\n::: {style=\"text-align: center; text-decoration: underline;\"}\n### Unsupervised Machine Learning\n:::\n\nUnsupervised learning involves using algorithms to analyze and cluster unlabeled data, discovering hidden patterns without predefined labels.\n\n::::::: columns\n:::: {.column width=\"50%\"}\n::: {style=\"text-align: center;\"}\n#### Pros\n:::\n- Powerful for data exploration\n  - Uncovers hidden structures and patterns in data\n- Useful for dimensionality reduction\n  - Simplifies complex datasets, making them easier to visualize and interpret\n::::\n\n:::: {.column width=\"50%\"}\n::: {style=\"text-align: center;\"}\n#### Cons\n:::\n- Lower interpretability\n  - Patterns and structures are harder to understand without labels\n- Challenging model evaluation\n  - Difficult to assess model quality without clear metrics\n::::\n:::::::\n\n[Common Algorithms: Principal Component Analysis (PCA), K-Means Clustering, t-Distributed Stochastic Neighbor Embedding (t-SNE)]{.footer-font}\n\n# How Does Machine Learning Work?\n\nThe [UC Berkeley School of Information](https://ischoolonline.berkeley.edu/blog/what-is-machine-learning/) has defined 3 components of most supervised machine learning algorithms.\n\n## 1. A Decision Process:\n\nIn general, machine learning algorithms are used to make a [prediction or classification]{.highlight1}. Based on some input data. The algorithm will produce an [estimate about a pattern]{.highlight1} in the data.\n\n## 2. An Error Function:\n\nAn error function [evaluates the prediction]{.highlight1} of the model. If there are known examples, an error function can make a comparison to assess the performance of the model.\n\n## 3. A Model Optimization Process:\n\nWhen the model does a good job of matching the training data, it tweaks its settings to get better at classifying the actual data. This process of [“evaluate and optimize”]{.highlight1} happens over and over again, allowing the model to adjust itself automatically.\n\n# Mini Tutorial with the Iris Dataset!\n\nLet's dive into a hands-on example using the famous Iris dataset. This classic dataset is perfect for illustrating basic machine learning concepts.\n\n## Human Classification Exercise\n\nBefore we get into the technical details, let's start with a quick exercise. A while ago, I was hiking in Duncan's Cove and came across some irises. I used an app called \"Seek,\" which leverages machine learning to identify species using your phone's camera. Curious to see how well it worked, I tested it on an iris I found. But before I reveal the app's result, I want you to take a look for yourself!\n\n### What do you think?\n\nTake a moment to compare the iris on the left with the Northern Blue Flag iris (middle) and the Beach Head iris (right). Which one do you think it resembles more? Consider what influenced your decision—was it the shape, color, or perhaps some other feature?\n\n::::: {layout=\"[1,1,1]\"}\n![My Picture from Duncan's Cove](images/duncans_iris_zoom.jpg){width=300 height=250}\n\n![\"Northern Blue Flag\" - Versicolor](images/northern%20blue%20flag%20(Iris%20versicolor)_files2.jpg){width=300 height=250}\n\n![\"Beach-Head\" - Setosa](images/Beach-head%20Iris%20(iris%20setosa)2.jpeg){width=300 height=250}\n:::::\n\n\n### Misclassification?\n\nInterestingly, the Seek App identified the iris I found as a Versicolor, but two experts later claimed it was a Setosa. This discrepancy highlights a critical aspect of machine learning: the [selection of features]{.highlight1}.\n\n![](images/inaturalist_pic.PNG){width=\"60%\" fig-align=\"center\"}\n\n## Choosing (the right) Features is Important!\n\nA [feature]{.highlight1} is \"an individual measurable property or characteristic of a phenomenon\".[^1] In the context of machine learning, selecting the right features is critical to the success of your model.\n\nUsing informative, discriminative, and independent features is essential for building effective classification algorithms. The features you choose can significantly influence the model’s ability to learn accurately from the data and make reliable predictions. In the case of our iris example, perhaps the app's algorithm relied on certain features that led it to misclassify the species.\n\n[^1]: [Bishop, Christopher (2006). *Pattern recognition and machine learning*](https://en.wikipedia.org/wiki/Feature_(machine_learning)#cite_note-ml-1)\n\n## The Iris Dataset\n\nThe Iris dataset is a classic example used in machine learning, containing three types of iris flowers: Setosa, Versicolor, and Virginica.\n\n![](images/iris_ex.png){width=\"85%\" fig-align=\"center\"}\n\nThe species of each iris can be determined by a combination of features, specifically the [petal and sepal width and length]{.highlight1}.\n\n## Taking Another Look\n\nGiven the features we’ve discussed, do you think the Seek AI might have relied on different features or interpreted them differently, leading to the incorrect classification?\n\n::::: {layout=\"[1,1,1]\"}\n![My Picture from Duncan's Cove](images/duncans_iris_zoom.jpg){width=300 height=250}\n\n![\"Northern Blue Flag\" - Versicolor](images/northern%20blue%20flag%20(Iris%20versicolor)_files2.jpg){width=300 height=250}\n\n![\"Beach-Head\" - Setosa](images/Beach-head%20Iris%20(iris%20setosa)2.jpeg){width=300 height=250}\n:::::\n\n## Let's Explore the Dataset!\n\nTo better understand how machine learning models work, let’s take a closer look at the Iris dataset.\n\n:::::: columns\n::: {.column width=\"50%\"}\n\n::: {#cell-Iris-dataset .cell execution_count=2}\n\n::: {#iris-dataset .cell-output .cell-output-display execution_count=2}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Variables</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>sepal_length</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>sepal_width</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>petal_length</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>petal_width</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>species</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td># Observations: 150</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n:::\n\n:::: {.column width=\"50%\"}\n::: {style=\"font-size: 30px;\"}\n-   4 Features (Independent Variables)\n-   150 Total Observations\n    -   50 of each Iris Type\n-   Excellent sample to demonstrate machine learning!\n:::\n\n![](images/sepal-petal.png){width=\"45%\" fig-align=\"center\"}\n::::\n::::::\n\n### Visualizing the Raw Data\n\nTo start, let’s visualize the raw data from the Iris dataset to better understand the relationships between different features.\n\n::: {#cell-CorrPlot .cell execution_count=3}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show the code\"}\nplt.style.use(\"dark_background\")\n\n# Making a correlation plot of all variables except 'Id'\ncorr_plot = sns.pairplot(data.iloc[:, 0:5], \n                         kind=\"scatter\", \n                         hue=\"species\", \n                         markers=[\"o\", \"s\", \"D\"], \n                         palette=\"Set2\", \n                         plot_kws={'alpha': 0.7})\n\nsns.move_legend(corr_plot, \"lower center\",\n    bbox_to_anchor=(0.5, 1), ncol=3, title=None, frameon=False\n)\n\n# Resize to fit\ncorr_plot.fig.set_size_inches(10, 5.5)\n\n# Show the plot\ncorr_plot\n```\n\n::: {.cell-output .cell-output-display}\n![](intro_to_ml_files/figure-html/corrplot-output-1.png){#corrplot width=838 height=586 fig-align='center'}\n:::\n:::\n\n\nThis plot offers a clear visualization of how different features are interrelated across the three iris species. Notice how, in various panels, each species tends to form its own distinct grouping, highlighting the separability of the data based on the selected features.\n\n### Reducing Dimentionality\n\nDimensionality reduction techniques, like Principal Component Analysis (PCA), help us simplify complex datasets. By translating our 4-dimensional data into two dimensions, we can better visualize the differences between species.\n\nHere, we can see how each species is relatively clustered when visualized with PCA.\n\n::: {#bcf17861 .cell execution_count=4}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show the code\"}\nplt.figure(figsize=(10, 5.5))\n\ndf = px.data.iris()\nX = df[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']]\n\npca = PCA(n_components=2)\ncomponents = pca.fit_transform(X)\n\n# Create a DataFrame with the PCA results\npca_df = pd.DataFrame(data=components, columns=['PCA1', 'PCA2'])\npca_df['species'] = df['species']\n\n# Plot the PCA results\npca2d = sns.scatterplot(data=pca_df, x='PCA1', y='PCA2', hue='species', palette=\"Set2\")\n```\n\n::: {.cell-output .cell-output-display}\n![](intro_to_ml_files/figure-html/cell-5-output-1.png){width=823 height=471 fig-align='center'}\n:::\n:::\n\n\n## Seperating Our Dependent and Independent Variables\n\nNext, let’s separate our dataset into independent (X/Input) and dependent (y/Output) variables. This is a crucial step before training our model, as it ensures that the model learns the relationships between the input features and the output labels without being biased by the answers themselves. We don’t want our model to use the labels (answers) during training, as that would defeat the purpose of learning from the data.\n\nHere, we take our original dataset of 150 observations, each with 4 features (petal & sepal length and width), and split them into independent variables (X), which the model will use to make predictions, and dependent variables (y), which represent the correct species classification for each observation (labels). This separation is essential for supervised learning because it allows the model to understand how different combinations of features correspond to specific outcomes.\n\n::: {#ind-dep1 .cell .panel-center execution_count=5}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show the code\"}\nX = data.drop(['species_id', 'species'], axis=1)\ny = data['species']\n```\n:::\n\n\n::: {#ind-dep2 .cell class='output-style3' execution_count=6}\n\n::: {.cell-output .cell-output-stdout}\n```\nThe feature (X/independent) variables are of shape: (150, 4)\nThe target (y/dependent) variables are of shape: (150,)\n```\n:::\n:::\n\n\n::::: columns\n::: {.column width=\"50%\"}\n\n::: {#cell-Ind-Dep3 .cell .panel-center class='output-style2' execution_count=7}\n\n::: {#ind-dep3 .cell-output .cell-output-display execution_count=7}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Independent Vars</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>sepal_length</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>sepal_width</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>petal_length</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>petal_width</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n:::\n\n::: {.column width=\"50%\"}\n\n::: {#cell-Ind-Dep4 .cell .panel-center class='output-style2' execution_count=8}\n\n::: {#ind-dep4 .cell-output .cell-output-display execution_count=8}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>species</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>111</th>\n      <td>virginica</td>\n    </tr>\n    <tr>\n      <th>74</th>\n      <td>versicolor</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>setosa</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>setosa</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n:::\n:::::\n\n## Train-Test Split\n\nThe classifier's performance can become inflated if it trains on the same data it is tested with, a problem known as [data leakage]{.highlight1}. Data leakage occurs when the model has access to information it shouldn't have during training, leading to overly optimistic performance metrics.\n\nTo evaluate model performance accurately and avoid this issue, it's essential to split the data into [training and testing sets]{.highlight1}. The training set is used to teach the model, while the testing set is used to evaluate how well the model generalizes to [unseen data]{.highlight1}.\n\nThis step is crucial because it helps us understand how the model will perform in real-world scenarios, where it will encounter data it has never seen before.\n\n![](images/train-test-split-1.png){fig-align=\"center\"}\n\n### Making the Train-Test Split\n\nLet’s proceed with the train-test split, keeping 30% of the data for testing and ensuring the class distribution remains consistent through [stratification]{.highlight1}.\n-   **Test size**: 30% of the data is reserved for testing the model.\n-   **Stratify**: Preserves the class ratios (33% of each iris species).\n\n::: {#train-test-split1 .cell execution_count=9}\n``` {.python .cell-code}\nX_train, X_test, y_train, y_test = train_test_split(X, y, # Input our seperated X and y data\n                                                    test_size=0.3, # Select the test size of 30%\n                                                    stratify=y, # Stratify the split by y (species/dependent)\n                                                    random_state=42) # Select a random state for reproducability\n```\n:::\n\n\nThis outputs:\n\n- `X_train`: 70% of our independent data (petal & sepal length and width)\n- `y_train`: 70% of our dependent data (species)\n- `X_test`: 30% of our independent data (petal & sepal length and width)\n- `y_test`: 30% of our dependent data (species)\n\n::: {#train-test-split2 .cell .panel-center class='output-style3' execution_count=10}\n\n::: {.cell-output .cell-output-stdout}\n```\nX_train shape is: (105, 4)\ny_train shape is: (105,) \n\nX_test shape is: (45, 4)\ny_test shape is: (45,)\n```\n:::\n:::\n\n\n## Why is Stratification Important?\n\nStratification is crucial because it ensures that each class is proportionally represented in both the training and testing sets.\n\nWithout stratification, the model might over-focus on more prevalent classes, leading to [biased performance and poor accuracy on underrepresented classes]{.highlight1}.\n\n::: {#cell-Stratifcation .cell .panel-center execution_count=11}\n\n::: {.cell-output .cell-output-display}\n![](intro_to_ml_files/figure-html/stratifcation-output-1.png){#stratifcation width=733 height=372}\n:::\n:::\n\n\nHere, we can visualize the difference between an unstratified and a stratified train-test split. If we were to train our model on the unstratified data, it would have less data on Iris Setosa to train with and more to test with, leading to an imbalance. This imbalance could cause the model to perform well on certain classes while underperforming on others, resulting in biased predictions. By using stratification, we ensure that each class is adequately represented in both the training and testing sets, leading to a more balanced and fair evaluation of the model’s performance.\n\n## Training the Models!\n\nNow that our data is ready, let’s train several supervised machine learning models and evaluate their performance. We'll be using the [scikit-learn Python library](https://scikit-learn.org/stable/index.html), which provides a wide range of tools for building and evaluating models. For a complete list of all the supervised models that scikit-learn supports, [click here](https://scikit-learn.org/stable/supervised_learning.html).\n\n\n\n::: {#model-training .cell execution_count=13}\n``` {.python .cell-code}\n# Instantiate the classifiers\nclassifiers = [                                     # Makes a list of classifier objects:\n    SVC(kernel='linear', random_state=42),             # Linear Support-Vector Machine (SVM)\n    SVC(kernel='rbf', random_state=42),                # RBF Support-Vector Machine (SVM)\n    DecisionTreeClassifier(random_state=42),           # Decision Tree \n    RandomForestClassifier(random_state=42),           # Random Forest \n    KNeighborsClassifier(),                            # K-nearest Neighbours \n    LogisticRegression(random_state=42, max_iter=1000) # Logistic Regression\n]\n\n# Train the classifiers\nfor clf in classifiers:          # Loop iterates through the list of classifiers\n    clf.fit(X_train, y_train)    # Trains with the X (input) and Y (output) training data\n    y_pred = clf.predict(X_test) # Fits the X test data; trained model returns predictions\n    accuracy = accuracy_score(y_test, y_pred) # Calculates accuracy of the model\n    print(f\"Accuracy of {clf.__class__.__name__}: {accuracy:.2f}\") # Prints the accuracy score\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy of SVC: 1.00\nAccuracy of SVC: 0.96\nAccuracy of DecisionTreeClassifier: 0.93\nAccuracy of RandomForestClassifier: 0.89\nAccuracy of KNeighborsClassifier: 0.98\nAccuracy of LogisticRegression: 0.93\n```\n:::\n:::\n\n\nHere, we get accuracy scores ranging from around 0.90 to 1.00 for each classifier. But what does this mean? That’s a great question! We’ll dive into the significance of these accuracy scores and how to properly evaluate the performance of machine learning models in the next section.\n\n## Visualizing Machine Learning!\n\nLet’s take a closer look at how well the trained models differentiate between the different iris species by visualizing their decision boundaries.\n\nTo make this possible, I applied Principal Component Analysis (PCA) to reduce our 4-dimensional data into 2 dimensions before training the models. This dimensionality reduction allows us to clearly visualize the decision boundaries each model establishes.\n\nIn the visualization, you can observe how each model draws boundaries that separate the iris species into distinct regions. For instance, if a new data point —say, an Iris Setosa— falls within the blue region, the model predicts that this iris is indeed an Iris Setosa. This approach helps us understand how the models make predictions based on the learned patterns in the data. However, if an Iris Virginica were incorrectly placed in another region, it would result in a misclassification. Such errors could prompt the machine learning model to adjust and refine its decision boundaries to improve accuracy.\n\n![](images/combined_PCA_decision_boundaries_with_accuracy2.png){fig-align=\"center\"}\n\n\n## Evaluating Model Performance\n\n### Accuracy Score\n\nEvaluation metrics are crucial for understanding how well your model is performing. The most commonly used metric is [Accuracy]{.highlight1}, which is defined as the number of correct predictions over the total number of predictions:\n$$\n\\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}}\n$$\n\nDo you recall the accuracy scores of the models we trained earlier? Since we tested each trained model on 45 unseen cases (the testing set), we can evaluate how well each model performed!\n\n<details>\n<summary> <span class=dropdown1> Click Here if You Forgot! </span> </summary>\n\n::: {#print-accuracy-scores .cell execution_count=14}\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy of SVC: 1.00\nAccuracy of SVC: 0.96\nAccuracy of DecisionTreeClassifier: 0.93\nAccuracy of RandomForestClassifier: 0.89\nAccuracy of KNeighborsClassifier: 0.98\nAccuracy of LogisticRegression: 0.93\n```\n:::\n:::\n\n\n</details>\n\nFor example, the Support Vector Classifier (SVC) had an accuracy of 1.00, meaning it correctly classified all 45 test inputs. On the other hand, the Random Forest Classifier had an accuracy score of 0.89. If we do some math, we can calculate the exact number of correctly classified cases for the Random Forest model:\n$$\n\\text{Number of Correct Predictions} = \\text{Accuracy} \\times \\text{Total Number of Test Cases}\n$$\n\n$$\n\\text{Number of Correct Predictions} = 0.89 \\times 45 \\approx 40\n$$\n\nThis means that out of 45 test cases, the Random Forest Classifier correctly classified 40 of them, with the remaining 5 cases being misclassified—which is still quite good!\n\n### Confusion Matrices\n\nA [confusion matrix]{.highlight1} is an excellent tool for visualizing your model's performance by providing a detailed breakdown of its predictions. Each row of the matrix represents the actual class, while each column represents the predicted class. The diagonal elements show correct predictions, while off-diagonal elements indicate misclassifications. Unlike accuracy alone, a confusion matrix shows how well your model distinguishes between different classes by displaying the counts of true positives, true negatives, false positives, and false negatives.\n\n<details>\n<summary> <span class=dropdown1> Click Here for a Breakdown of Error Types </span> </summary>\n\n- **True Positives (TP):** These are cases where the model correctly predicted the positive class. For example, if the model predicts \"Iris Setosa\" and it’s actually \"Iris Setosa,\" that’s a true positive.\n- **True Negatives (TN):** These are cases where the model correctly predicted the negative class. For example, if the model predicts \"Not Iris Setosa\" and it’s indeed not \"Iris Setosa,\" that’s a true negative.\n- **False Positives (FP):** Also known as Type I errors, these occur when the model incorrectly predicts the positive class. For instance, if the model predicts \"Iris Setosa\" when it’s actually a different species, that’s a false positive.\n- **False Negatives (FN):** Also known as Type II errors, these occur when the model incorrectly predicts the negative class. For example, if the model fails to predict \"Iris Setosa\" and instead predicts another species when it is actually \"Iris Setosa,\" that’s a false negative.\n\nThis detailed breakdown is crucial for understanding the types of errors your model is making. In the context of a BCI-controlled wheelchair, false negatives could mean failing to detect the user's command to move, which might be more critical than a false positive. For example, if the system fails to recognize a user's intention to move the wheelchair forward, it could leave the user stranded or unable to navigate effectively. On the other hand, a false positive might cause the wheelchair to move unintentionally, which could be disorienting or even dangerous, depending on the situation.\n</details>\n\n:::::::: columns\n:::: {.column width=\"50%\"}\n![](images/Linear_SVM_CM.png)\n\n45 Correct Predictions / 45 Total Prediction = 1.00\n::::\n\n::::: {.column width=\"50%\"}\n![](images/DecisionTree_CM.png)\n\n43 Correct Predictions / 45 Total Prediction = 0.95\n\n:::::\n::::::::\n\nFor example, in the case of the Iris dataset, the confusion matrix reveals not just the overall accuracy but also whether your model is prone to specific errors, such as predicting one species of iris when it’s actually another (false positives) or failing to correctly identify a species when it’s present (false negatives). This detailed view helps you understand how well your model distinguishes between the three iris species—Setosa, Versicolor, and Virginica.\n\nThis level of detail is particularly valuable when dealing with imbalanced datasets, where one class may be much more prevalent than the other. In such cases, a high accuracy might be misleading, while the confusion matrix reveals the true performance of your model across all classes.\n\nBy analyzing the confusion matrix, you can gain deeper insights into the strengths and weaknesses of your model, guiding you toward improvements, such as tweaking the model, adjusting thresholds, or selecting different metrics like Precision, Recall, or F1 Score for a more comprehensive evaluation.\n\nOverall, accuracy is a valuable metric for evaluating how well your model predicts or classifies data. It can also provide guidance on which model may work best for your specific dataset. However, while accuracy is useful, it’s not always the best metric, especially in cases where the classes are imbalanced.\n\n## When is Accuracy Sub-optimal?\n\nIn cases of [imbalanced classes]{.highlight1}, accuracy can be misleading. Let’s explore why this is the case.\n\nImagine we have a trained model that we want to test with new data. Suppose this new dataset has 100 observations: 80 are of Iris Setosa, and 20 are of Iris Versicolor.\n\n::::::: columns\n::: {.column width=\"45%\"}\n![](images/imbalanced-dataset2.png){fig-align=\"center\"}\n:::\n\n::: {.column width=\"5%\"}\n:::\n\n::::: {.column width=\"50%\"}\n\nIf the classifier predicted 'Setosa' for all 100 observations, what would the accuracy be?\n\n$$\n\\scriptsize{\\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}}}\n$$\n\n80 Correct Predictions / 100 Total Predictions = 0.80\n\nSo, even though the accuracy score is a respectable 80%, the model isn’t actually distinguishing between the two classes at all!\n\n:::::\n:::::::\n\nIn situations where there is an imbalance of classes, accuracy does not provide a fully informative picture of the model's performance, as illustrated in the example above. Metrics like F1 Score, Precision, and Recall (Sensitivity) can offer a more comprehensive assessment by considering the balance between different types of errors.\n\n## Summary\n\n- **Labeled Data:** Supervised machine learning relies on labeled data, which is particularly effective for classification tasks, allowing models to learn and make accurate predictions.\n- **Feature Selection:** Choosing the right features is crucial for model success. Informative and discriminative features help the model make reliable predictions.\n- **Train-Test Split:** Splitting your data into training and testing sets is essential to prevent data leakage and ensure that your model's performance is evaluated on unseen data.\n- **Stratification:** Stratifying your data during the train-test split ensures that each class is proportionally represented, leading to more balanced and accurate model evaluations.\n- **Accuracy as a Metric:** While accuracy is a valuable metric, it may not always be sufficient, particularly in cases of imbalanced classes. Complementary metrics like F1 Score, Precision, and Recall provide a more comprehensive assessment of model performance.\n\n",
    "supporting": [
      "intro_to_ml_files\\figure-html"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}