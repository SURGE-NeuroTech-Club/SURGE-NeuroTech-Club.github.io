[
  {
    "objectID": "who_we_are.html",
    "href": "who_we_are.html",
    "title": "Who We Are",
    "section": "",
    "text": "We are the Dalhousie/SURGE NeuroTech Club! The goal of the club is to help students with any level of neuroscience or coding experience get engaged and excited about NeuroTechnology!\nThe club is drop-in and completely free to attend! It runs once a week from 3-5pm on Thursdays in the LSC, Oceanography - Room O2660"
  },
  {
    "objectID": "who_we_are.html#directions",
    "href": "who_we_are.html#directions",
    "title": "Who We Are",
    "section": "Directions",
    "text": "Directions\nLife Sciences Building, Oceanography wing - Room O2660\nFrom the main entrance: go down the stairs and continue past the tim hortons until you see doors leading outside on your right. Go through them and enter the building directly across. Continue straight down the hallway until you see the bathrooms on your left, or the SURGE A-frame sign - then it’s the doors on your right!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "SURGE NeuroTech Club",
    "section": "",
    "text": "We are a community of neuro-enthusiasts dedicated to exploring, understanding, and innovating in the field of neurotechnology. Join us as we delve into this exciting frontier!\nWe meet every Thursday 3-5 pm in the SURGE room (LSC, Room O2660). If you can’t make the full time, no worries - feel free to drop by and check it out!\nIf you’re new to the club and don’t know where to begin, you can click on Get Started or, just ask one of the SURGE folks!"
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "SURGE NeuroTech Club",
    "section": "Contact:",
    "text": "Contact:\nFor general inquiries, email us at: bciclub@dal.ca\nFor specific neurotech questions, please reach out to our programming specialists: Brynn or Max"
  },
  {
    "objectID": "docs/tutorials/machine-learning/intro_to_ml.html",
    "href": "docs/tutorials/machine-learning/intro_to_ml.html",
    "title": "A Beginner’s Guide to Machine Learning Concepts and Applications",
    "section": "",
    "text": "Machine learning is a subset of artificial intelligence where computers use data to identify patterns and make predictions. These models improve over time by learning from new data and experiences.\nMachine learning algorithms work by identifying patterns in the data, specifically between the independent variables (features, often represented as X) and the dependent variables (targets, often represented as Y). These patterns enable the model to make predictions or classifications based on new, unseen data."
  },
  {
    "objectID": "docs/tutorials/machine-learning/intro_to_ml.html#what-is-machine-learning",
    "href": "docs/tutorials/machine-learning/intro_to_ml.html#what-is-machine-learning",
    "title": "A Beginner’s Guide to Machine Learning Concepts and Applications",
    "section": "",
    "text": "Machine learning is a subset of artificial intelligence where computers use data to identify patterns and make predictions. These models improve over time by learning from new data and experiences.\nMachine learning algorithms work by identifying patterns in the data, specifically between the independent variables (features, often represented as X) and the dependent variables (targets, often represented as Y). These patterns enable the model to make predictions or classifications based on new, unseen data."
  },
  {
    "objectID": "docs/tutorials/machine-learning/intro_to_ml.html#labeled-unlabeled-data",
    "href": "docs/tutorials/machine-learning/intro_to_ml.html#labeled-unlabeled-data",
    "title": "A Beginner’s Guide to Machine Learning Concepts and Applications",
    "section": "Labeled & Unlabeled Data",
    "text": "Labeled & Unlabeled Data\nBefore we get into machine learning, we need to talk about data. Data is the foundation of all machine learning—without it, we wouldn’t be able to train the models! There are two broad categories of data used in machine learning: labeled and unlabeled data.\nLabeled data contains both the input and the corresponding correct output/target or dependent measure. In this example, we can see two types of labeled data—one labeled with “cat/dog” and another labeled with “weight.” In each case, the model would be trained to predict whether the input was a cat or a dog, or to estimate the weight, depending on the label provided. Unlabeled data only contains the input, leaving the model to identify patterns without guidance."
  },
  {
    "objectID": "docs/tutorials/machine-learning/intro_to_ml.html#two-category-of-machine-learning",
    "href": "docs/tutorials/machine-learning/intro_to_ml.html#two-category-of-machine-learning",
    "title": "A Beginner’s Guide to Machine Learning Concepts and Applications",
    "section": "Two Category of Machine Learning",
    "text": "Two Category of Machine Learning\nMachine learning can be broadly divided into two primary categories: supervised and unsupervised learning. Each approach has its own strengths and limitations, making them suitable for different types of tasks.\n\n\n\nSupervised Learning\n\n\nUses labeled training data\nMajority of machine learning applications\nCommonly used for tasks like classification and regression\n\n\n\nUnsupervised Learning\n\n\nUses unlabeled training data\nLess common but powerful for exploratory analysis\nOften used for clustering, content personalization, and dimensionality reduction\n\n\n\n\nSupervised Machine Learning\n\nSupervised machine learning involves training models on labeled data, where the goal is often to make predictions or classifications based on new, unseen data.\n\n\n\nPros\n\n\nHigh accuracy\n\nAbility to learn from known examples\n\nExcellent predictive power\n\nEffective at classifying new data based on historical patterns\n\n\n\n\nCons\n\n\nRequires labeled data\n\nTime-consuming and expensive to obtain\n\nRisk of overfitting\n\nModels may become too tailored to the training data\n\n\n\n\nCommon Algorithms: Linear/Logistic Regression, Support Vector Machines, Decision Trees, K-nearest neighbours\n\nUnsupervised Machine Learning\n\nUnsupervised learning involves using algorithms to analyze and cluster unlabeled data, discovering hidden patterns without predefined labels.\n\n\n\nPros\n\n\nPowerful for data exploration\n\nUncovers hidden structures and patterns in data\n\nUseful for dimensionality reduction\n\nSimplifies complex datasets, making them easier to visualize and interpret\n\n\n\n\nCons\n\n\nLower interpretability\n\nPatterns and structures are harder to understand without labels\n\nChallenging model evaluation\n\nDifficult to assess model quality without clear metrics\n\n\n\n\nCommon Algorithms: Principal Component Analysis (PCA), K-Means Clustering, t-Distributed Stochastic Neighbor Embedding (t-SNE)"
  },
  {
    "objectID": "docs/tutorials/machine-learning/intro_to_ml.html#a-decision-process",
    "href": "docs/tutorials/machine-learning/intro_to_ml.html#a-decision-process",
    "title": "A Beginner’s Guide to Machine Learning Concepts and Applications",
    "section": "1. A Decision Process:",
    "text": "1. A Decision Process:\nIn general, machine learning algorithms are used to make a prediction or classification. Based on some input data. The algorithm will produce an estimate about a pattern in the data."
  },
  {
    "objectID": "docs/tutorials/machine-learning/intro_to_ml.html#an-error-function",
    "href": "docs/tutorials/machine-learning/intro_to_ml.html#an-error-function",
    "title": "A Beginner’s Guide to Machine Learning Concepts and Applications",
    "section": "2. An Error Function:",
    "text": "2. An Error Function:\nAn error function evaluates the prediction of the model. If there are known examples, an error function can make a comparison to assess the performance of the model."
  },
  {
    "objectID": "docs/tutorials/machine-learning/intro_to_ml.html#a-model-optimization-process",
    "href": "docs/tutorials/machine-learning/intro_to_ml.html#a-model-optimization-process",
    "title": "A Beginner’s Guide to Machine Learning Concepts and Applications",
    "section": "3. A Model Optimization Process:",
    "text": "3. A Model Optimization Process:\nWhen the model does a good job of matching the training data, it tweaks its settings to get better at classifying the actual data. This process of “evaluate and optimize” happens over and over again, allowing the model to adjust itself automatically."
  },
  {
    "objectID": "docs/tutorials/machine-learning/intro_to_ml.html#human-classification-exercise",
    "href": "docs/tutorials/machine-learning/intro_to_ml.html#human-classification-exercise",
    "title": "A Beginner’s Guide to Machine Learning Concepts and Applications",
    "section": "Human Classification Exercise",
    "text": "Human Classification Exercise\nBefore we get into the technical details, let’s start with a quick exercise. A while ago, I was hiking in Duncan’s Cove and came across some irises. I used an app called “Seek,” which leverages machine learning to identify species using your phone’s camera. Curious to see how well it worked, I tested it on an iris I found. But before I reveal the app’s result, I want you to take a look for yourself!\n\nWhat do you think?\nTake a moment to compare the iris on the left with the Northern Blue Flag iris (middle) and the Beach Head iris (right). Which one do you think it resembles more? Consider what influenced your decision—was it the shape, color, or perhaps some other feature?\n\n\n\n\n\n\n\n\n\nMy Picture from Duncan’s Cove\n\n\n\n\n\n\n\n“Northern Blue Flag” - Versicolor\n\n\n\n\n\n\n\n“Beach-Head” - Setosa\n\n\n\n\n\n\n\nMisclassification?\nInterestingly, the Seek App identified the iris I found as a Versicolor, but two experts later claimed it was a Setosa. This discrepancy highlights a critical aspect of machine learning: the selection of features."
  },
  {
    "objectID": "docs/tutorials/machine-learning/intro_to_ml.html#choosing-the-right-features-is-important",
    "href": "docs/tutorials/machine-learning/intro_to_ml.html#choosing-the-right-features-is-important",
    "title": "A Beginner’s Guide to Machine Learning Concepts and Applications",
    "section": "Choosing (the right) Features is Important!",
    "text": "Choosing (the right) Features is Important!\nA Feature is “an individual measurable property or characteristic of a phenomenon”.1 In the context of machine learning, selecting the right features is critical to the success of your model.\nUsing informative, discriminative, and independent features is essential for building effective classification algorithms. The features you choose can significantly influence the model’s ability to learn accurately from the data and make reliable predictions. In the case of our iris example, perhaps the app’s algorithm relied on certain features that led it to misclassify the species."
  },
  {
    "objectID": "docs/tutorials/machine-learning/intro_to_ml.html#the-iris-dataset",
    "href": "docs/tutorials/machine-learning/intro_to_ml.html#the-iris-dataset",
    "title": "A Beginner’s Guide to Machine Learning Concepts and Applications",
    "section": "The Iris Dataset",
    "text": "The Iris Dataset\nThe Iris dataset is a classic example used in machine learning, containing three types of iris flowers: Setosa, Versicolor, and Virginica.\n\n\n\n\n\nThe species of each iris can be determined by a combination of features, specifically the petal and sepal width and length."
  },
  {
    "objectID": "docs/tutorials/machine-learning/intro_to_ml.html#taking-another-look",
    "href": "docs/tutorials/machine-learning/intro_to_ml.html#taking-another-look",
    "title": "A Beginner’s Guide to Machine Learning Concepts and Applications",
    "section": "Taking Another Look",
    "text": "Taking Another Look\nGiven the features we’ve discussed, do you think the Seek AI might have relied on different features or interpreted them differently, leading to the incorrect classification?\n\n\n\n\n\n\n\n\n\nMy Picture from Duncan’s Cove\n\n\n\n\n\n\n\n“Northern Blue Flag” - Versicolor\n\n\n\n\n\n\n\n“Beach-Head” - Setosa"
  },
  {
    "objectID": "docs/tutorials/machine-learning/intro_to_ml.html#lets-explore-the-dataset",
    "href": "docs/tutorials/machine-learning/intro_to_ml.html#lets-explore-the-dataset",
    "title": "A Beginner’s Guide to Machine Learning Concepts and Applications",
    "section": "Let’s Explore the Dataset!",
    "text": "Let’s Explore the Dataset!\nTo better understand how machine learning models work, let’s take a closer look at the Iris dataset.\n\n\n\n\n\n\n\n\n\n\n\nVariables\n\n\n\n\n0\nsepal_length\n\n\n1\nsepal_width\n\n\n2\npetal_length\n\n\n3\npetal_width\n\n\n4\nspecies\n\n\n5\n# Observations: 150\n\n\n\n\n\n\n\n\n\n\n4 Features (Independent Variables)\n150 Total Observations\n\n50 of each Iris Type\n\nExcellent sample to demonstrate machine learning!\n\n\n\n\n\n\n\n\n\n\nVisualizing the Raw Data\nTo start, let’s visualize the raw data from the Iris dataset to better understand the relationships between different features.\n\n\nShow the code\nplt.style.use(\"dark_background\")\n\n# Making a correlation plot of all variables except 'Id'\ncorr_plot = sns.pairplot(data.iloc[:, 0:5], \n                         kind=\"scatter\", \n                         hue=\"species\", \n                         markers=[\"o\", \"s\", \"D\"], \n                         palette=\"Set2\", \n                         plot_kws={'alpha': 0.7})\n\nsns.move_legend(corr_plot, \"lower center\",\n    bbox_to_anchor=(0.5, 1), ncol=3, title=None, frameon=False\n)\n\n# Resize to fit\ncorr_plot.fig.set_size_inches(10, 5.5)\n\n# Show the plot\ncorr_plot\n\n\n\n\n\n\n\n\n\nThis plot offers a clear visualization of how different features are interrelated across the three iris species. Notice how, in various panels, each species tends to form its own distinct grouping, highlighting the separability of the data based on the selected features.\n\n\nReducing Dimentionality\nDimensionality reduction techniques, like Principal Component Analysis (PCA), help us simplify complex datasets. By translating our 4-dimensional data into two dimensions, we can better visualize the differences between species.\nHere, we can see how each species is relatively clustered when visualized with PCA.\n\n\nShow the code\nplt.figure(figsize=(10, 5.5))\n\ndf = px.data.iris()\nX = df[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']]\n\npca = PCA(n_components=2)\ncomponents = pca.fit_transform(X)\n\n# Create a DataFrame with the PCA results\npca_df = pd.DataFrame(data=components, columns=['PCA1', 'PCA2'])\npca_df['species'] = df['species']\n\n# Plot the PCA results\npca2d = sns.scatterplot(data=pca_df, x='PCA1', y='PCA2', hue='species', palette=\"Set2\")"
  },
  {
    "objectID": "docs/tutorials/machine-learning/intro_to_ml.html#seperating-our-dependent-and-independent-variables",
    "href": "docs/tutorials/machine-learning/intro_to_ml.html#seperating-our-dependent-and-independent-variables",
    "title": "A Beginner’s Guide to Machine Learning Concepts and Applications",
    "section": "Seperating Our Dependent and Independent Variables",
    "text": "Seperating Our Dependent and Independent Variables\nNext, let’s separate our dataset into independent (X/Input) and dependent (y/Output) variables. This is a crucial step before training our model, as it ensures that the model learns the relationships between the input features and the output labels without being biased by the answers themselves. We don’t want our model to use the labels (answers) during training, as that would defeat the purpose of learning from the data.\nHere, we take our original dataset of 150 observations, each with 4 features (petal & sepal length and width), and split them into independent variables (X), which the model will use to make predictions, and dependent variables (y), which represent the correct species classification for each observation (labels). This separation is essential for supervised learning because it allows the model to understand how different combinations of features correspond to specific outcomes.\n\n\n\n\nShow the code\nX = data.drop(['species_id', 'species'], axis=1)\ny = data['species']\n\n\n\n\n\n\nThe feature (X/independent) variables are of shape: (150, 4)\nThe target (y/dependent) variables are of shape: (150,)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIndependent Vars\n\n\n\n\n0\nsepal_length\n\n\n1\nsepal_width\n\n\n2\npetal_length\n\n\n3\npetal_width\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nspecies\n\n\n\n\n34\nsetosa\n\n\n17\nsetosa\n\n\n25\nsetosa\n\n\n109\nvirginica"
  },
  {
    "objectID": "docs/tutorials/machine-learning/intro_to_ml.html#train-test-split",
    "href": "docs/tutorials/machine-learning/intro_to_ml.html#train-test-split",
    "title": "A Beginner’s Guide to Machine Learning Concepts and Applications",
    "section": "Train-Test Split",
    "text": "Train-Test Split\nThe classifier’s performance can become inflated if it trains on the same data it is tested with, a problem known as data leakage. Data leakage occurs when the model has access to information it shouldn’t have during training, leading to overly optimistic performance metrics.\nTo evaluate model performance accurately and avoid this issue, it’s essential to split the data into training and testing sets. The training set is used to teach the model, while the testing set is used to evaluate how well the model generalizes to unseen data.\nThis step is crucial because it helps us understand how the model will perform in real-world scenarios, where it will encounter data it has never seen before.\n\n\n\n\n\n\nMaking the Train-Test Split\nLet’s proceed with the train-test split, keeping 30% of the data for testing and ensuring the class distribution remains consistent through stratification. - Test size: 30% of the data is reserved for testing the model. - Stratify: Preserves the class ratios (33% of each iris species).\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, # Input our seperated X and y data\n                                                    test_size=0.3, # Select the test size of 30%\n                                                    stratify=y, # Stratify the split by y (species/dependent)\n                                                    random_state=42) # Select a random state for reproducability\n\nThis outputs:\n\nX_train: 70% of our independent data (petal & sepal length and width)\ny_train: 70% of our dependent data (species)\nX_test: 30% of our independent data (petal & sepal length and width)\ny_test: 30% of our dependent data (species)\n\n\n\n\n\nX_train shape is: (105, 4)\ny_train shape is: (105,) \n\nX_test shape is: (45, 4)\ny_test shape is: (45,)"
  },
  {
    "objectID": "docs/tutorials/machine-learning/intro_to_ml.html#why-is-stratification-important",
    "href": "docs/tutorials/machine-learning/intro_to_ml.html#why-is-stratification-important",
    "title": "A Beginner’s Guide to Machine Learning Concepts and Applications",
    "section": "Why is Stratification Important?",
    "text": "Why is Stratification Important?\nStratification is crucial because it ensures that each class is proportionally represented in both the training and testing sets.\nWithout stratification, the model might over-focus on more prevalent classes, leading to biased performance and poor accuracy on underrepresented classes.\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere, we can visualize the difference between an unstratified and a stratified train-test split. If we were to train our model on the unstratified data, it would have less data on Iris Setosa to train with and more to test with, leading to an imbalance. This imbalance could cause the model to perform well on certain classes while underperforming on others, resulting in biased predictions. By using stratification, we ensure that each class is adequately represented in both the training and testing sets, leading to a more balanced and fair evaluation of the model’s performance."
  },
  {
    "objectID": "docs/tutorials/machine-learning/intro_to_ml.html#training-the-models",
    "href": "docs/tutorials/machine-learning/intro_to_ml.html#training-the-models",
    "title": "A Beginner’s Guide to Machine Learning Concepts and Applications",
    "section": "Training the Models!",
    "text": "Training the Models!\nNow that our data is ready, let’s train several supervised machine learning models and evaluate their performance. We’ll be using the scikit-learn Python library, which provides a wide range of tools for building and evaluating models. For a complete list of all the supervised models that scikit-learn supports, click here.\n\n# Instantiate the classifiers\nclassifiers = [                                     # Makes a list of classifier objects:\n    SVC(kernel='linear', random_state=42),             # Linear Support-Vector Machine (SVM)\n    SVC(kernel='rbf', random_state=42),                # RBF Support-Vector Machine (SVM)\n    DecisionTreeClassifier(random_state=42),           # Decision Tree \n    RandomForestClassifier(random_state=42),           # Random Forest \n    KNeighborsClassifier(),                            # K-nearest Neighbours \n    LogisticRegression(random_state=42, max_iter=1000) # Logistic Regression\n]\n\n# Train the classifiers\nfor clf in classifiers:          # Loop iterates through the list of classifiers\n    clf.fit(X_train, y_train)    # Trains with the X (input) and Y (output) training data\n    y_pred = clf.predict(X_test) # Fits the X test data; trained model returns predictions\n    accuracy = accuracy_score(y_test, y_pred) # Calculates accuracy of the model\n    print(f\"Accuracy of {clf.__class__.__name__}: {accuracy:.2f}\") # Prints the accuracy score\n\nAccuracy of SVC: 1.00\nAccuracy of SVC: 0.96\nAccuracy of DecisionTreeClassifier: 0.93\nAccuracy of RandomForestClassifier: 0.89\nAccuracy of KNeighborsClassifier: 0.98\nAccuracy of LogisticRegression: 0.93\n\n\nHere, we get accuracy scores ranging from around 0.90 to 1.00 for each classifier. But what does this mean? That’s a great question! We’ll dive into the significance of these accuracy scores and how to properly evaluate the performance of machine learning models in the next section."
  },
  {
    "objectID": "docs/tutorials/machine-learning/intro_to_ml.html#visualizing-machine-learning",
    "href": "docs/tutorials/machine-learning/intro_to_ml.html#visualizing-machine-learning",
    "title": "A Beginner’s Guide to Machine Learning Concepts and Applications",
    "section": "Visualizing Machine Learning!",
    "text": "Visualizing Machine Learning!\nLet’s take a closer look at how well the trained models differentiate between the different iris species by visualizing their decision boundaries.\nTo make this possible, I applied Principal Component Analysis (PCA) to reduce our 4-dimensional data into 2 dimensions before training the models. This dimensionality reduction allows us to clearly visualize the decision boundaries each model establishes.\nIn the visualization, you can observe how each model draws boundaries that separate the iris species into distinct regions. For instance, if a new data point —say, an Iris Setosa— falls within the blue region, the model predicts that this iris is indeed an Iris Setosa. This approach helps us understand how the models make predictions based on the learned patterns in the data. However, if an Iris Virginica were incorrectly placed in another region, it would result in a misclassification. Such errors could prompt the machine learning model to adjust and refine its decision boundaries to improve accuracy."
  },
  {
    "objectID": "docs/tutorials/machine-learning/intro_to_ml.html#evaluating-model-performance",
    "href": "docs/tutorials/machine-learning/intro_to_ml.html#evaluating-model-performance",
    "title": "A Beginner’s Guide to Machine Learning Concepts and Applications",
    "section": "Evaluating Model Performance",
    "text": "Evaluating Model Performance\n\nAccuracy Score\nEvaluation metrics are crucial for understanding how well your model is performing. The most commonly used metric is Accuracy, which is defined as the number of correct predictions over the total number of predictions: \\[\n\\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}}\n\\]\nDo you recall the accuracy scores of the models we trained earlier? Since we tested each trained model on 45 unseen cases (the testing set), we can evaluate how well each model performed!\n\n\n Click Here if You Forgot! \n\n\n\nAccuracy of SVC: 1.00\nAccuracy of SVC: 0.96\nAccuracy of DecisionTreeClassifier: 0.93\nAccuracy of RandomForestClassifier: 0.89\nAccuracy of KNeighborsClassifier: 0.98\nAccuracy of LogisticRegression: 0.93\n\n\n\nFor example, the Support Vector Classifier (SVC) had an accuracy of 1.00, meaning it correctly classified all 45 test inputs. On the other hand, the Random Forest Classifier had an accuracy score of 0.89. If we do some math, we can calculate the exact number of correctly classified cases for the Random Forest model: \\[\n\\text{Number of Correct Predictions} = \\text{Accuracy} \\times \\text{Total Number of Test Cases}\n\\]\n\\[\n\\text{Number of Correct Predictions} = 0.89 \\times 45 \\approx 40\n\\]\nThis means that out of 45 test cases, the Random Forest Classifier correctly classified 40 of them, with the remaining 5 cases being misclassified—which is still quite good!\n\n\nConfusion Matrices\nA confusion matrix is an excellent tool for visualizing your model’s performance by providing a detailed breakdown of its predictions. Each row of the matrix represents the actual class, while each column represents the predicted class. The diagonal elements show correct predictions, while off-diagonal elements indicate misclassifications. Unlike accuracy alone, a confusion matrix shows how well your model distinguishes between different classes by displaying the counts of true positives, true negatives, false positives, and false negatives.\n\n\n Click Here for a Breakdown of Error Types \n\n\nTrue Positives (TP): These are cases where the model correctly predicted the positive class. For example, if the model predicts “Iris Setosa” and it’s actually “Iris Setosa,” that’s a true positive.\nTrue Negatives (TN): These are cases where the model correctly predicted the negative class. For example, if the model predicts “Not Iris Setosa” and it’s indeed not “Iris Setosa,” that’s a true negative.\nFalse Positives (FP): Also known as Type I errors, these occur when the model incorrectly predicts the positive class. For instance, if the model predicts “Iris Setosa” when it’s actually a different species, that’s a false positive.\nFalse Negatives (FN): Also known as Type II errors, these occur when the model incorrectly predicts the negative class. For example, if the model fails to predict “Iris Setosa” and instead predicts another species when it is actually “Iris Setosa,” that’s a false negative.\n\nThis detailed breakdown is crucial for understanding the types of errors your model is making. In the context of a BCI-controlled wheelchair, false negatives could mean failing to detect the user’s command to move, which might be more critical than a false positive. For example, if the system fails to recognize a user’s intention to move the wheelchair forward, it could leave the user stranded or unable to navigate effectively. On the other hand, a false positive might cause the wheelchair to move unintentionally, which could be disorienting or even dangerous, depending on the situation.\n\n\n\n\n45 Correct Predictions / 45 Total Prediction = 1.00\n\n\n43 Correct Predictions / 45 Total Prediction = 0.95\n\n\nFor example, in the case of the Iris dataset, the confusion matrix reveals not just the overall accuracy but also whether your model is prone to specific errors, such as predicting one species of iris when it’s actually another (false positives) or failing to correctly identify a species when it’s present (false negatives). This detailed view helps you understand how well your model distinguishes between the three iris species—Setosa, Versicolor, and Virginica.\nThis level of detail is particularly valuable when dealing with imbalanced datasets, where one class may be much more prevalent than the other. In such cases, a high accuracy might be misleading, while the confusion matrix reveals the true performance of your model across all classes.\nBy analyzing the confusion matrix, you can gain deeper insights into the strengths and weaknesses of your model, guiding you toward improvements, such as tweaking the model, adjusting thresholds, or selecting different metrics like Precision, Recall, or F1 Score for a more comprehensive evaluation.\nOverall, accuracy is a valuable metric for evaluating how well your model predicts or classifies data. It can also provide guidance on which model may work best for your specific dataset. However, while accuracy is useful, it’s not always the best metric, especially in cases where the classes are imbalanced."
  },
  {
    "objectID": "docs/tutorials/machine-learning/intro_to_ml.html#when-is-accuracy-sub-optimal",
    "href": "docs/tutorials/machine-learning/intro_to_ml.html#when-is-accuracy-sub-optimal",
    "title": "A Beginner’s Guide to Machine Learning Concepts and Applications",
    "section": "When is Accuracy Sub-optimal?",
    "text": "When is Accuracy Sub-optimal?\nIn cases of imbalanced classes, accuracy can be misleading. Let’s explore why this is the case.\nImagine we have a trained model that we want to test with new data. Suppose this new dataset has 100 observations: 80 are of Iris Setosa, and 20 are of Iris Versicolor.\n\n\n\n\n\n\n\n\n\n\nIf the classifier predicted ‘Setosa’ for all 100 observations, what would the accuracy be?\n\\[\n\\scriptsize{\\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}}}\n\\]\n80 Correct Predictions / 100 Total Predictions = 0.80\nSo, even though the accuracy score is a respectable 80%, the model isn’t actually distinguishing between the two classes at all!\n\n\nIn situations where there is an imbalance of classes, accuracy does not provide a fully informative picture of the model’s performance, as illustrated in the example above. Metrics like F1 Score, Precision, and Recall (Sensitivity) can offer a more comprehensive assessment by considering the balance between different types of errors."
  },
  {
    "objectID": "docs/tutorials/machine-learning/intro_to_ml.html#summary",
    "href": "docs/tutorials/machine-learning/intro_to_ml.html#summary",
    "title": "A Beginner’s Guide to Machine Learning Concepts and Applications",
    "section": "Summary:",
    "text": "Summary:\n\nLabeled Data: Supervised machine learning relies on labeled data, which is particularly effective for classification tasks, allowing models to learn and make accurate predictions.\nFeature Selection: Choosing the right features is crucial for model success. Informative and discriminative features help the model make reliable predictions.\nTrain-Test Split: Splitting your data into training and testing sets is essential to prevent data leakage and ensure that your model’s performance is evaluated on unseen data.\nStratification: Stratifying your data during the train-test split ensures that each class is proportionally represented, leading to more balanced and accurate model evaluations.\nAccuracy as a Metric: While accuracy is a valuable metric, it may not always be sufficient, particularly in cases of imbalanced classes. Complementary metrics like F1 Score, Precision, and Recall provide a more comprehensive assessment of model performance."
  },
  {
    "objectID": "docs/tutorials/machine-learning/intro_to_ml.html#footnotes",
    "href": "docs/tutorials/machine-learning/intro_to_ml.html#footnotes",
    "title": "A Beginner’s Guide to Machine Learning Concepts and Applications",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBishop, Christopher (2006). Pattern recognition and machine learning↩︎"
  },
  {
    "objectID": "docs/tutorials/get-started/github_intro.html",
    "href": "docs/tutorials/get-started/github_intro.html",
    "title": "Introduction to GitHub",
    "section": "",
    "text": "GitHub is a web-based platform for version control and collaboration that allows developers to host and manage their code. It uses Git, a distributed version control system, to track changes in source code during software development. GitHub provides a range of features including issue tracking, project management, and code review tools."
  },
  {
    "objectID": "docs/tutorials/get-started/github_intro.html#what-is-github",
    "href": "docs/tutorials/get-started/github_intro.html#what-is-github",
    "title": "Introduction to GitHub",
    "section": "",
    "text": "GitHub is a web-based platform for version control and collaboration that allows developers to host and manage their code. It uses Git, a distributed version control system, to track changes in source code during software development. GitHub provides a range of features including issue tracking, project management, and code review tools."
  },
  {
    "objectID": "docs/tutorials/get-started/github_intro.html#key-features-of-github",
    "href": "docs/tutorials/get-started/github_intro.html#key-features-of-github",
    "title": "Introduction to GitHub",
    "section": "Key Features of GitHub",
    "text": "Key Features of GitHub\n\nRepositories: Storage spaces where your project files and their revision history are kept.\nCommits: Saved changes to files in a repository, allowing you to track modifications.\nBranches: Separate lines of development within a repository, useful for working on different features simultaneously.\nPull Requests: Proposals to merge changes from one branch to another, often used for code reviews and collaboration.\nIssues: Tools for tracking tasks, enhancements, and bugs in your projects."
  },
  {
    "objectID": "docs/tutorials/get-started/github_intro.html#getting-started-with-github",
    "href": "docs/tutorials/get-started/github_intro.html#getting-started-with-github",
    "title": "Introduction to GitHub",
    "section": "Getting Started with GitHub",
    "text": "Getting Started with GitHub\nBefore you get started here, make sure you already have a GitHub account setup. If you don’t head over to Getting Setup for instructions!\n\n1. Installing GitHub Desktop\n\nDownload GitHub Desktop from desktop.github.com if you don’t already have it installed.\nInstall GitHub Desktop following the instructions for your operating system.\n\n\n\n2. Cloning a Repository\nTo work on your project locally, clone the repository to your computer using GitHub Desktop.\n\nOpen GitHub Desktop.\nClick File &gt; Clone repository.\nSelect the repository you want to clone from the list or enter the URL of the repository.\nChoose the local path where you want to save the repository and click Clone.\n\nExplanation: Cloning a repository means creating a local copy of the repository on your computer, allowing you to work on it offline and sync your changes with the remote repository later.\n\n\n3. Creating a Branch\n99% of the time, you’ll want to work on a Branch. This effectively copies the code from main and allows you to work on it separately. This is great for a few reasons: - It avoids conflicts with overwriting which can happen when multiple people work on the same code in the same branch. - If you seriously break something and can’t figure out how to get it back - you can just pull from main!\nTo create a branch:\n\nSelect the repository you want to make a branch of.\nClick Current Branch &gt; New Branch.\nName the branch whatever you want (make sure to remember it!).\nSelect where you want the code to copy from (Main is the default branch, but you can copy the code from someone else’s branch too).\nClick Create Branch.\nMake sure the branch you just made is selected under Current Branch.\n\nExplanation: Branches allow you to develop features, fix bugs, or safely experiment with new ideas in isolation from the main codebase.\n\n\n4. Making Changes and Committing\n\nOpen the cloned repository in your preferred code editor and make changes to your files.\nGo back to GitHub Desktop. You will see your changes listed under Changes.\nAdd a summary and description for your commit in the Summary and Description fields.\nClick Commit to main (or your branch name if you are using a different one).\n\nExplanation: Committing saves your changes to the local repository, with a message describing what you did. This helps in keeping track of different versions of your code.\n\n\n5. Pushing Changes to GitHub\n\nAfter committing your changes, click Push origin at the top of GitHub Desktop to upload your commits to GitHub.\n\nExplanation: Pushing sends your committed changes to the remote repository on GitHub, making them available to others.\n\n\n6. Creating a Pull Request\nA pull request is a proposal to merge changes from one branch to another. It’s used for code review and collaboration before integrating changes into the main branch. Once you feel like you’ve implemented what you intended, you can make a pull request to merge your changes with the main (or another) branch.\n\nNavigate to your repository on GitHub.\nClick on the Pull requests tab.\nClick New pull request.\nSelect the branch you want to merge from and to.\nAdd a title and description for your pull request.\nClick Create pull request.\n\nExplanation: Pull requests facilitate discussion about your changes before they are merged into the main branch, ensuring code quality and collaboration."
  },
  {
    "objectID": "docs/tutorials/get-started/github_intro.html#conclusion",
    "href": "docs/tutorials/get-started/github_intro.html#conclusion",
    "title": "Introduction to GitHub",
    "section": "Conclusion",
    "text": "Conclusion\nGitHub is a powerful platform for version control and collaboration, enabling developers to work together on projects effectively. By creating repositories, making commits, and managing pull requests with GitHub Desktop, you can maintain a smooth and organized workflow.\nAnother great resource to check out is the NCIL Handbook’s section on GitHub"
  },
  {
    "objectID": "docs/tutorials/brain-computer-interfaces/intro_to_bci.html",
    "href": "docs/tutorials/brain-computer-interfaces/intro_to_bci.html",
    "title": "Introduction to Brain-Computer Interfaces",
    "section": "",
    "text": "A Brain-Computer Interface (BCI) enables communication between the brain and an external device, such as a computer or robotic arm, without physical movement. BCIs allow users to control devices using their thoughts."
  },
  {
    "objectID": "docs/tutorials/brain-computer-interfaces/intro_to_bci.html#what-is-a-brain-computer-interface-bci",
    "href": "docs/tutorials/brain-computer-interfaces/intro_to_bci.html#what-is-a-brain-computer-interface-bci",
    "title": "Introduction to Brain-Computer Interfaces",
    "section": "",
    "text": "A Brain-Computer Interface (BCI) enables communication between the brain and an external device, such as a computer or robotic arm, without physical movement. BCIs allow users to control devices using their thoughts."
  },
  {
    "objectID": "docs/tutorials/brain-computer-interfaces/intro_to_bci.html#how-does-a-bci-work",
    "href": "docs/tutorials/brain-computer-interfaces/intro_to_bci.html#how-does-a-bci-work",
    "title": "Introduction to Brain-Computer Interfaces",
    "section": "How Does a BCI Work?",
    "text": "How Does a BCI Work?\n\nThe Brain and Its Signals\nThe brain is composed of billions of neurons that communicate through electrical signals. BCIs detect and interpret these signals, often using electroencephalography (EEG), which measures electrical activity through the scalp.\n\n\n Learn More about Neurons and Brain Signals \n\n\nNeurons are specialized cells that transmit information throughout the nervous system. They communicate via electrical impulses and chemical signals. Each neuron consists of a cell body, dendrites, and an axon. Dendrites receive incoming signals, while the axon transmits signals to other neurons.\nNeurons generate action potentials, which are brief electrical impulses resulting from changes in membrane potential. These action potentials travel along the axon to the synapse, where neurotransmitters are released to signal adjacent neurons.\nEEG measures the synchronous activity of neurons, primarily in the cortex. This activity results in detectable electrical signals on the scalp, representing the brain’s response to stimuli and cognitive processes.\n\n\n\nProducing and Reading Brain Signals\nEEG signals arise from the synchronized activity of cortical pyramidal neurons when users focus on specific stimuli. BCI systems use machine learning algorithms to learn and interpret brain signals into commands.\n\n\n Understanding Electric Dipoles and EEG Detection \n\n\nPyramidal neurons in the cortex are key contributors to EEG signals. These neurons have long apical dendrites oriented perpendicularly to the cortical surface, forming electric dipoles when they fire.\nThe combined electric fields of many neurons create measurable potentials on the scalp. EEG electrodes placed on the scalp detect these potentials, recording voltage fluctuations over time.\nMachine learning algorithms process the EEG data, identifying patterns associated with specific mental states or intentions - such as those arising from attending to a specific stimulus. This enables translation of brain activity into actionable commands for BCIs.\n\n\n\nTranslating Thoughts into Action\nMachine learning algorithms distinguish brain signals generated by focusing on specific stimuli from background noise, translating them into commands for device control. This enables users to perform tasks like moving a cursor, playing a game, or controlling a robotic arm solely through thought.\n\n\n Exploring Machine Learning in BCIs \n\n Machine learning algorithms in BCIs analyze EEG data to identify patterns (like event-related potentials or steady-state visually-evoked potentials) corresponding to user intentions (such as attending to a certain stimulus). These algorithms are trained on labeled datasets to distinguish between when random noise and when the user is attending a stimulus.\nThe most common technique is supervised learning, where models learn from labeled examples. Other techniques can be successfully used such as unsupervised learning, which identifies hidden patterns without explicit labels, and deep learning, a subset of machine learning, employs neural networks to extract complex features from EEG signals.\nBy continuously adapting to user inputs, machine learning enhances the accuracy and responsiveness of BCIs, enabling seamless interaction between the brain and external devices."
  },
  {
    "objectID": "docs/tutorials/brain-computer-interfaces/intro_to_bci.html#two-common-neural-responses-used-for-bci",
    "href": "docs/tutorials/brain-computer-interfaces/intro_to_bci.html#two-common-neural-responses-used-for-bci",
    "title": "Introduction to Brain-Computer Interfaces",
    "section": "Two Common Neural Responses Used for BCI",
    "text": "Two Common Neural Responses Used for BCI\n\nEvent-Related Potentials (ERPs)\nERPs are brain responses time-locked to specific events. They consist of components like the commonly-used P300, which occur about 300 milliseconds after an infrequent or important stimulus. In ERP-based BCIs, users focus on stimuli, and the system detects the P300 responses to determine which stimulus the user is looking at, and therefore which command to execute.\n\n\n\nExample Event-Related Potentials\n\n\n\n\n Diving into ERP Components and Their Significance \n\n ERP components are labeled based on their polarity (positive or negative) and timing. Key components include:\n\nP300: A positive deflection occurring around 300 milliseconds after an infrequent or significant stimulus. In BCIs, the P300 is used to detect when a user is focusing on a particular stimulus, such as a specific letter in a matrix. The system then translates this detection into a command, enabling the user to select that letter or perform an action.\nN200: A negative deflection associated with response inhibition and conflict detection. In BCIs, the N200 can be used to monitor error detection or user responses to conflicting stimuli, which can be integrated into more sophisticated control systems.\nN400: Related to language processing and semantic incongruence. While less commonly used in BCIs, the N400 could assist in applications involving language or communication tasks by detecting how users process and respond to language-based stimuli.\n\nERPs provide insights into cognitive processes and brain responses, making them valuable for understanding attention, perception, and decision-making.\n\n\n\nSteady-State Visual Evoked Potentials (SSVEPs)\nSSVEPs are responses to flickering visual stimuli, causing detectible brain waves at the same frequency. In SSVEP-based BCIs, users focus on a flickering stimulus to synchronize brain activity with its frequency, allowing for the machine learning algorithm to determine which stimulus the user is focused on, and then execute the corresponding command.\n\n\n Delving into SSVEPs and Frequency Analysis \n\n SSVEPs are generated when the visual cortex responds to stimuli flickering at specific frequencies. The brain’s response frequency matches the stimulus frequency, enabling reliable detection with EEG.\nIn SSVEP-based BCIs, users focus on one of several flickering stimuli, each associated with a distinct command. The system identifies the frequency of the brain’s response to determine user intent. (For example, if the user attends to a stimulus flickering at 10 Hz, the resulting EEG signals would show a 10 Hz oscillation, and the machine learning algorithm would perform the command corresponding to the stimulus flickering at 10 Hz)\nFrequency analysis involves decomposing EEG signals into their frequency components. Techniques such as Fourier Transform and Wavelet Transform help isolate SSVEP frequencies for accurate interpretation.\n\n\n\nComparison and Considerations\n\nSpeed and Accuracy: SSVEP-based BCIs generally offer faster and more accurate responses due to direct frequency matching.\nUser Comfort: ERP-based systems may be less visually demanding, as SSVEPs require focus on flickering stimuli, which can cause fatigue.\nSignal Processing: Both require advanced techniques to interpret user intentions from EEG data.\n\n\n\nNote on Hybrid BCIs\nHybrid BCIs combine multiple neural responses, such as ERPs and SSVEPs, to enhance the performance and versatility of the system. By leveraging the strengths of different neural signals, hybrid BCIs can provide more robust and reliable control, especially in complex or dynamic environments.\nHow Hybrid BCIs Work: - Integration of Multiple Signals: Hybrid BCIs simultaneously monitor different types of brain signals, such as combining the time-locked precision of ERPs with the continuous frequency information from SSVEPs. This integration allows the system to cross-validate user intentions, improving accuracy. - Increased Command Options: By using both ERPs and SSVEPs, hybrid BCIs can offer a wider range of commands or actions. For example, an ERP might be used to select a menu item, while an SSVEP could determine how to interact with it. - Enhanced Adaptability: Hybrid BCIs can adapt to varying user states or environmental conditions. If one signal type becomes less reliable (e.g., if a user becomes fatigued and SSVEP detection declines), the system can rely more heavily on the other signal type.\n\n\n Advantages and Applications of Hybrid BCIs \n\n\nAdvantages: - Improved Accuracy: The combination of multiple neural signals reduces the likelihood of errors by cross-referencing different types of brain activity. - Greater Flexibility: Users can perform a broader range of actions, as the system can interpret more diverse types of commands. - Resilience: Hybrid BCIs can maintain functionality even if one type of signal is compromised, making them more reliable in real-world applications.\n\nHybrid BCIs represent an exciting frontier in brain-computer interface technology, combining the best of multiple approaches to create more effective and user-friendly systems. As research and development continue, hybrid BCIs are likely to become increasingly prominent in both clinical and non-clinical applications."
  },
  {
    "objectID": "docs/tutorials/brain-computer-interfaces/intro_to_bci.html#applications-of-bcis",
    "href": "docs/tutorials/brain-computer-interfaces/intro_to_bci.html#applications-of-bcis",
    "title": "Introduction to Brain-Computer Interfaces",
    "section": "Applications of BCIs",
    "text": "Applications of BCIs\nBCIs have transformative potential across fields:\n\nMedical: Helping individuals with disabilities control wheelchairs or prosthetic limbs.\nCommunication: Allowing those unable to speak to convert thoughts into text or speech.\nGaming and Entertainment: Creating immersive experiences controlled by thought.\nResearch and Exploration: Studying brain functions and developing neurological treatments.\n\n\n\n Exploring BCI Applications in More Detail \n\n\n\nMedical Applications: BCIs restore independence for individuals with mobility impairments, enabling them to control assistive devices through thought alone. For example, BCIs can be integrated with robotic prosthetics to facilitate movement for amputees or individuals with paralysis.\nCommunication: BCIs enable individuals with communication disorders to express themselves by converting brain signals into text or speech, enhancing social interaction and quality of life.\nGaming and Entertainment: BCIs offer unique gaming experiences, where players control in-game actions using their thoughts, creating a more immersive and interactive environment.\nResearch: BCIs facilitate research into brain function and cognitive processes, providing insights into neural mechanisms underlying behavior and aiding the development of treatments for neurological disorders."
  },
  {
    "objectID": "docs/tutorials/brain-computer-interfaces/intro_to_bci.html#the-future-of-bcis",
    "href": "docs/tutorials/brain-computer-interfaces/intro_to_bci.html#the-future-of-bcis",
    "title": "Introduction to Brain-Computer Interfaces",
    "section": "The Future of BCIs",
    "text": "The Future of BCIs\nBCI technology is evolving rapidly, with ongoing research aiming to make systems more accessible and user-friendly, expanding their impact on how we interact with technology.\n\n\n Innovations and Future Directions in BCIs \n\n\n\nImproved Accessibility: Researchers are working on developing affordable and easy-to-use BCI systems, making them accessible to a wider population.\nEnhanced Signal Quality: Advances in signal processing and electrode technology aim to improve the quality and reliability of EEG signals, enhancing BCI performance.\nIntegration with AI: Combining BCIs with artificial intelligence allows for more sophisticated interpretation of brain signals, improving the accuracy and speed of communication and control.\nWearable BCIs: Future BCIs may be integrated into wearable devices, offering seamless and unobtrusive interaction with technology in everyday life."
  },
  {
    "objectID": "docs/tutorials/brain-computer-interfaces/intro_to_bci.html#conclusion",
    "href": "docs/tutorials/brain-computer-interfaces/intro_to_bci.html#conclusion",
    "title": "Introduction to Brain-Computer Interfaces",
    "section": "Conclusion",
    "text": "Conclusion\nBrain-Computer Interfaces merge neuroscience and technology, enabling direct communication between the brain and devices. They have the potential to revolutionize various fields, making the world more inclusive and innovative."
  },
  {
    "objectID": "docs/tutorials/brain-computer-interfaces/intro_to_bci.html#further-reading",
    "href": "docs/tutorials/brain-computer-interfaces/intro_to_bci.html#further-reading",
    "title": "Introduction to Brain-Computer Interfaces",
    "section": "Further Reading",
    "text": "Further Reading\nGeneral Neuroscience:\n\nResearch Methods in Cognitive Neuroscience [Book] - Dr. Aaron Newman\n\nGeneral BCI:\n\nProgress in Brain Computer Interface: Challenges and Opportunities - Saha et al., 2017\nCurrent Status, Challenges, and Possible Solutions of EEG-Based Brain-Computer Interface: A Comprehensive Review - Rashid et al., 2020\nData Analytics in Steady-State Visual Evoked Potential-Based Brain–Computer Interface: A Review - Zhang et al., 2021\nHybrid brain–computer interface spellers: A walkthrough recent advances in signal processing methods and challenges. International Journal of Human–Computer Interaction - Chugh, N. & Aggarwal, S., 2022"
  },
  {
    "objectID": "docs/tutorials/brain-computer-interfaces/intro_to_bci.html#references",
    "href": "docs/tutorials/brain-computer-interfaces/intro_to_bci.html#references",
    "title": "Introduction to Brain-Computer Interfaces",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "contact_us.html",
    "href": "contact_us.html",
    "title": "Contact Us",
    "section": "",
    "text": "Email us at: bciclub@dal.ca"
  },
  {
    "objectID": "docs/tutorials/get-started/getting_setup.html",
    "href": "docs/tutorials/get-started/getting_setup.html",
    "title": "Getting Setup",
    "section": "",
    "text": "Click to learn more about VS Code! \n\n\nVisual Studio Code (often referred to as VS Code) is a free, open-source code editor developed by Microsoft. It’s lightweight, yet powerful, and supports development in multiple programming languages such as Python, JavaScript, Java, C++, and many more. VS Code features include syntax highlighting, intelligent code completion (IntelliSense), code refactoring, and debugging tools. It also supports Git for version control, has a built-in terminal for command-line operations, and a rich ecosystem of extensions for enhancing its functionality. It’s designed to be highly customizable, allowing users to change the editor’s theme, keyboard shortcuts, and preferences. VS Code is available for Windows, macOS, and Linux. It’s widely used by developers across the globe for both small and large scale projects.\n\n\n\n\nFirst, you’ll want to download and install the latest version of Visual Studio Code. Make sure you download the correct version for your computer (Windows, Mac, Linux).\nFollow the steps in the dialog windows, and once installed, you next need to download some essential extensions. On the left side of the window, there will be a bar with some icons. Select the extensions icon:  From there, search for and install the following: (Or just click the links below and hit ‘install’)\n\nGitHub Copilot & GitHub Copilot Chat\nPython\nJupyter\nAny others that seem interesting or useful :)\n\nFor more information on how to use VSCode, see the resources at the bottom of this page"
  },
  {
    "objectID": "docs/tutorials/get-started/getting_setup.html#step-1-setting-up-visual-studio-code",
    "href": "docs/tutorials/get-started/getting_setup.html#step-1-setting-up-visual-studio-code",
    "title": "Getting Setup",
    "section": "",
    "text": "Click to learn more about VS Code! \n\n\nVisual Studio Code (often referred to as VS Code) is a free, open-source code editor developed by Microsoft. It’s lightweight, yet powerful, and supports development in multiple programming languages such as Python, JavaScript, Java, C++, and many more. VS Code features include syntax highlighting, intelligent code completion (IntelliSense), code refactoring, and debugging tools. It also supports Git for version control, has a built-in terminal for command-line operations, and a rich ecosystem of extensions for enhancing its functionality. It’s designed to be highly customizable, allowing users to change the editor’s theme, keyboard shortcuts, and preferences. VS Code is available for Windows, macOS, and Linux. It’s widely used by developers across the globe for both small and large scale projects.\n\n\n\n\nFirst, you’ll want to download and install the latest version of Visual Studio Code. Make sure you download the correct version for your computer (Windows, Mac, Linux).\nFollow the steps in the dialog windows, and once installed, you next need to download some essential extensions. On the left side of the window, there will be a bar with some icons. Select the extensions icon:  From there, search for and install the following: (Or just click the links below and hit ‘install’)\n\nGitHub Copilot & GitHub Copilot Chat\nPython\nJupyter\nAny others that seem interesting or useful :)\n\nFor more information on how to use VSCode, see the resources at the bottom of this page"
  },
  {
    "objectID": "docs/tutorials/get-started/getting_setup.html#step-2-setting-up-github",
    "href": "docs/tutorials/get-started/getting_setup.html#step-2-setting-up-github",
    "title": "Getting Setup",
    "section": "Step 2: Setting up GitHub",
    "text": "Step 2: Setting up GitHub\n\n\nClick to learn more about GitHub! \n\n\nGitHub is a web-based platform used for version control and collaboration. It allows multiple people to work on projects at once without overwriting each other’s changes. GitHub is built on Git, a distributed version control system that tracks changes to files. With GitHub, you can manage and store revisions of projects, share your code with others, view and track changes, and even revert back to previous versions of your code. It’s widely used by software developers for personal projects, open-source projects, and team-based enterprise software development.\n\n\n\nMaking Your Account\nHead over to GitHub.com and make an account if you don’t already have one. Make sure you use your Dalhousie email when signing up as this will allow you to sign up for the student developer pack which gives you access to AI tools like GitHub Copilot!\n\n\nGetting Student Perks\nOnce you’ve made and verified your account, head over to the GitHub Student Developer Pack Application. Make sure Student is selected, and then scroll down to the bottom of the page.\nIf you signed up with your Dalhousie email address, it should automatically detect Dalhousie University as your school. Click Select this School, and then Continue. If you don’t see this, enter your Dalhousie email address and verify it now. Note: Your browser may prompt you to share your location, this is required and you will not be allowed to continue until you agree.\n\n\n\n\n\nOn the next page, you will have to take a picture of your Dalhousie Student ID (I reccomend using your laptop webcam for simplicity), and upload it. Note: The website says you need to have an expiration date on your card, but since Dal student ID’s don’t have one you should be fine. If it keeps giving you errors, ask someone for assistance :)\n\n\n\n\n\n\nThis step can be a bit tricky and students sometimes get errors saying they require 2FA (2-factor authentication), in which case you will need to add your phone number or some other 2FA to your github account. If you have tried to troubleshoot on your own and keep getting stuck, just swing by the club and ask one of the SURGE folks - we’re happy to assist :)\n\nAfter you have completed the GitHub setup, you will get an email in a few hours/days saying that your application has been accepted. Once you receive this, you can open VSCode up, find the GitHub Copilot extensions and log into them. Now you have access to a powerful AI that can help you understand and write code!\n\n\nInstalling GitHub Desktop\nFinally, you will need to install GitHub Desktop. This application provides a friendly user-interface to work on GitHub repositories without having to learn the git bash (command line). Install that and log into your GitHub account.\nOnce this is done we highly reccomend you check out some further tutorials on what GitHub is, and how to effectively use it! There is a mini introduction to GitHub here, as well as other great resources at the bottom of this page"
  },
  {
    "objectID": "docs/tutorials/get-started/getting_setup.html#step-3-setting-up-a-virtual-environment",
    "href": "docs/tutorials/get-started/getting_setup.html#step-3-setting-up-a-virtual-environment",
    "title": "Getting Setup",
    "section": "Step 3: Setting up a Virtual Environment",
    "text": "Step 3: Setting up a Virtual Environment\n\n\nClick to learn more about Virtual Environments & Miniforge! \n\n\nVirtual environments are isolated spaces for installing and managing packages and dependencies for specific projects without affecting the global Python environment. They offer isolation, precise dependency management, portability, and a cleaner global environment. Virtual environments can be created using tools like venv or virtualenv in Python or Conda’s environment management features. By activating a virtual environment, users can install packages specific to that project, ensuring consistency and avoiding conflicts with other projects. This approach is essential for maintaining project-specific dependencies and ensuring reproducible and stable development environments.\n\n\nMiniforge3 is a minimalistic distribution of Conda, an open-source package and environment management system widely used in data science and software development. It provides a lightweight, cross-platform tool that supports multiple CPU architectures, making it versatile for different hardware and operating systems. Miniforge3 allows users to install packages from sources like conda-forge, ensuring access to a broad range of up-to-date software. Its primary strength lies in environment management, enabling the creation of isolated environments with specific dependencies, thereby avoiding conflicts and ensuring project stability.\n\n\n\nInstalling Miniforge3\nNext we need to get a python virtual environment setup so you can start coding! A virtual environment is an isolated workspace on a computer that allows projects to have their own dependencies and configurations, preventing conflicts and ensuring consistent and reproducible development and deployment. It is essential for managing multiple projects with different requirements on the same machine.\nTo start, head over to the conda/miniforge website and scroll down to download the version that matches your machine (windows, mac, linux, etc.)\nNext, open the installer and follow the steps. It isn’t necessary, but I reccomend selecting Install for all Users. Once the installation is complete you should be able to find a program called “Miniforge Prompt” or “Miniforge3” on your desktop or by searching for it.\n\n\nSetting Up Your Environment\nThe next step involved you cloning a repository to your computer so make sure you have your GitHub account & GitHub Desktop setup. In the GitHub Desktop App, in the top left, click on Current Repository &gt; Add &gt; Clone Existing Repository. Now either find the repository called “SURGE-NeuroTech-Club/virtual-environments” or if you can’t find it, click on URL and paste in: SURGE-NeuroTech-Club/virtual-environments. Before clicking Clone, copy the location of the folder it will clone to - you’ll need it soon.\nNext, you’ll want to open your Miniforge3 Prompt and use the cd command to change to the directory to where you cloned the repository. For example, type: cd \"the location to the repository you coppied\". If you aren’t sure where you cloned the repository, you can click on this button  in the GitHub desktop app and copy the path.\nYou will know you’re in the right directory when it reads something along the lines of: (base) C:\\..\\..\\virtual-environments. Once you see this, type in: mamba env create -f ncil.yml. This will read the list of libraries and packages within the ncil.yml file and begin to download and install them into a virtual environment for you. You may be prompted to answer Yes/No (Y/N) at some points during the installation process.\nOnce the install is complete, you can check that it worked by typing in: mamba activate ncil. This should change from (base) C:\\etc... to (ncil) C:\\etc....\n\n\nUsing Your Virtual Environment in VSCode\nNow that you have installed your virtual environment, you can use it in VSCode! To do this, all you have to do is open up VSCode and hit ctrl+shift+p (command+shift+p on mac), and type: Python: Select Interpreter. Then from the drop-down menu you should be able to see “Python 3.12.2 (‘ncil’)” and tada you can now execute python scripts using your brand-new virtual environment!"
  },
  {
    "objectID": "docs/tutorials/get-started/getting_setup.html#additional-resources",
    "href": "docs/tutorials/get-started/getting_setup.html#additional-resources",
    "title": "Getting Setup",
    "section": "Additional Resources",
    "text": "Additional Resources\nThe NCIL Lab Handbook has many great resources - Data Science Tools\nVSCode:\n\nIntroduction to VSCode\n\nGitHub:\n\nIntroduction to GitHub\nNCIL Introduction to GitHub"
  },
  {
    "objectID": "docs/tutorials/index.html",
    "href": "docs/tutorials/index.html",
    "title": "Welcome to the Get Started Landing Page!",
    "section": "",
    "text": "Welcome to the Get Started Landing Page!\n\nIf you’re brand new and want to get your computer setup to start working on projects, head over to Getting Setup.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA Beginner’s Guide to Machine Learning Concepts and Applications\n\n\nThis tutorial introduces the fundamentals of machine learning, covering key concepts such as supervised and unsupervised learning, data labeling, and model evaluation.\n\n\n\n\n\n\n\n\n\n\n\n\n\nGetting Setup\n\n\nThis document will teach you how to setup your computer with GitHub, VSCode and a Virtual Environment so you can start working on python projects!\n\n\n\n\n\n\n\n\n\n\n\nGetting Started with OpenBCI\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Brain-Computer Interfaces\n\n\nThis tutorial will teach you about the basics of what Brain-Computer Interfaces are, and how they work!\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to GitHub\n\n\nThis guide will give you a basic introduction to GitHub and how to effectively use it!\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "docs/tutorials/open-bci/open_bci_setup.html",
    "href": "docs/tutorials/open-bci/open_bci_setup.html",
    "title": "Getting Started with OpenBCI",
    "section": "",
    "text": "This tutorial will take you through the steps of getting you OpenBCI board up and streaming EEG signals!"
  },
  {
    "objectID": "docs/tutorials/open-bci/open_bci_setup.html#installing-the-software",
    "href": "docs/tutorials/open-bci/open_bci_setup.html#installing-the-software",
    "title": "Getting Started with OpenBCI",
    "section": "Installing The Software",
    "text": "Installing The Software\nFirst, you will have to download the latest version of the OpenBCI GUI (graphical user interface) Make sure to download the correct version for your machine! (windows, mac, or linux).\nOne you have downloaded it, unzip it and move the folder somewhere you will remember! &gt; The GUI doesn’t ‘install’ so you will have to open the folder to start the program\nOnce you open the application OpenBCI_GUI, you should see something like this: \nIf you see the red bar at the bottom that reads ‘This Application is Not being run with Admin Access…’, close it and restart it as an administrator (right click &gt; Run As Administrator &gt; Yes)\nIf you run into trouble or errors not mentioned here, check out the official GUI documentation (or ask one of the SURGE folks!)"
  },
  {
    "objectID": "docs/tutorials/open-bci/open_bci_setup.html#connecting-to-your-board",
    "href": "docs/tutorials/open-bci/open_bci_setup.html#connecting-to-your-board",
    "title": "Getting Started with OpenBCI",
    "section": "Connecting To Your Board",
    "text": "Connecting To Your Board\nIf at any point something doesn’t work or make sense - check out the official documentation on getting your cyton board setup!\nMake sure you have your Cyton Board, Dongle, and Battery Pack with 4 AA batteries. \nThere is a tiny switch on the side of the cyton board:  - make sure it is in the ‘off’ position (middle) before pluggin in the battery.\nOnce the switch is set to ‘off’, you may plug in the battery pack  and change the switch position to ‘PC’. If you do not see a blue light on the board once you switch it to ‘PC’, the batteries need to be replaced.\nNext, look for the switch on the USB dongle and **make sure the switch is on ‘GPIO 6’ - towards the plug-end before pluggin it into your computer."
  },
  {
    "objectID": "docs/tutorials/open-bci/open_bci_setup.html#more-information-references",
    "href": "docs/tutorials/open-bci/open_bci_setup.html#more-information-references",
    "title": "Getting Started with OpenBCI",
    "section": "More Information & References",
    "text": "More Information & References\nOpenBCI GUI Reference"
  },
  {
    "objectID": "site_info.html",
    "href": "site_info.html",
    "title": "Official Website of SURGE Innovation’s NeuroTech Club",
    "section": "",
    "text": "This is the official website of SURGE Innovation’s NeuroTech Club hosted by Dalhousie University\nWebsite made using Quarto & hosted via Github pages"
  }
]